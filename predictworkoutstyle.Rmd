---
title: "Predict Workout Style"
author: "Jayesh Gokhale"
date: "5/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predict Workout Style

Let us download the files first
```{r downloadfile}
if (!file.exists("pml-training.csv"))
{
  download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                destfile = "pml-training.csv",method="curl")
}

if (!file.exists("pml-testing.csv"))
{
  download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                destfile = "pml-testing.csv",method="curl")
}
```

```{r loadlibrary, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggpubr)
library(caret)
```

Let us load the data
```{r loaddata}
df.train <- read.csv("pml-training.csv")
df.test <- read.csv("pml-testing.csv")
```

The first thing is that this timestamp is some crazy julian date or something which I cannot make head or tail out of. Let us do one thing. Let us subtract the min from it.

```{r splitCV, echo=FALSE, eval=FALSE, warning=FALSE, results='hide', message=FALSE}
set.seed(12345)
trainIndex = createDataPartition(df.train.cv$classe, p = 0.8,list=FALSE)
df.train = df.train.cv[trainIndex,]
df.cv = df.train.cv[-trainIndex,]

nrow(df.train)
nrow(df.cv)
```

Exclude All NA vars. For now we will exclude the columns where more than 95% rows are NA.Let us have a look at them later
```{r findallNA}
areAllNA <- function(myvector,epsilon=1)
{
  if ((sum(is.na(myvector))/length(myvector)) >= epsilon) return(TRUE)
  return(FALSE)
}

getNAProportion <- function(myvector)
{
  na.proportion <- sum(is.na(myvector))/as.numeric(length(myvector))
  return(na.proportion)
}

allNACols <- apply(df.train,2,areAllNA,epsilon=0.95)
legit.Cols <- names(allNACols[allNACols==FALSE])

legit.Cols

df.train <- df.train[legit.Cols]
#df.test <- df.test[legit.Cols]
```

```{r removeuselessvars}
useless.vars <- c('kurtosis_yaw_belt',
'skewness_yaw_belt',
'amplitude_yaw_belt',
'kurtosis_yaw_dumbbell',
'skewness_yaw_dumbbell',
'amplitude_yaw_dumbbell',
'kurtosis_yaw_forearm',
'skewness_yaw_forearm',
'amplitude_yaw_forearm '
)

df.train <- df.train[!(colnames(df.train) %in% useless.vars)]

# These below values are treated as NA
df.train[df.train=="#DIV/0!"] <- NA
df.train[df.train==""] <- NA ##This is suspect move
```

Let us explore the data
```{r describedata}
summary(df.train)
str(df.train)
```

```{r anothersetofremoval}
allNACols.2 <- apply(df.train,2,areAllNA,epsilon=0.95)
legit.Cols.2 <- names(allNACols.2[allNACols.2==FALSE])


df.train <- df.train[legit.Cols.2]
```

Let us explore the data again
```{r describedata2}
summary(df.train)
str(df.train)
```

## Including Plots




Now the thing is that there are far too many variables observing each and every one of them manually would be extremely time consuming. Let us do one thing: Let us do a one way ANOVA to get F-Statistic and order these by decrease order of F-Statistic and pick only some top N variables.

```{r filtersignificant}
getSignificanceValue <- function(dfvar,classe)
{
  if(is.factor(dfvar)) return(-1)
  res.aov <- aov(dfvar ~ classe)
  f.statistic <- summary(res.aov)[[1]][1,4]
  return(f.statistic)
}

signif.vars <- lapply(df.train,getSignificanceValue,classe=df.train$classe)
signif.vars <- do.call("rbind",signif.vars)
signif.vars <- as.data.frame(signif.vars)
signif.vars$column_name <- rownames(signif.vars)
colnames(signif.vars) <- c("AOV_F_STATISTIC","COLUMN_NAME")
signif.vars <- signif.vars[c("COLUMN_NAME","AOV_F_STATISTIC")]
signif.vars <- signif.vars[order(-signif.vars$AOV_F_STATISTIC),]
signif.vars
```

```{r describedata3,eval=FALSE}
summary(df.train)
str(df.train)
```


Let us draw a sample box plot of first 6
```{r sampleboxplot}
getBoxPlot <- function(df,xvar,yvar,fillvar,xlabel,ylabel,filllabel)
{
  mybox <- ggplot(data=df, aes(x=xvar,y=yvar)) +
  geom_boxplot(outlier.colour="black", 
               outlier.size=2,position=position_dodge(1)) + 
  labs(title = "",
       subtitle = "",
       y = ylabel, x = xlabel) +
    scale_fill_brewer(name = filllabel,palette="Dark2")  +
  theme(plot.title = element_text(hjust = 0.5)) 
  
  return(mybox)
}

signif.vars.short <- signif.vars[2:7,]



for (i in 1:6)
{
  mytext <- paste0("plot",i," <-  getBoxPlot(df.train,df.train$classe,df.train$",signif.vars.short[i,]$COLUMN_NAME,
                   ",'','Class','",signif.vars.short[i,]$COLUMN_NAME,"','')")
  eval(parse(text=mytext))
  
  #print(mytext)
}
figure.box <- ggarrange(plot1, plot2, plot3, plot4,plot5,plot6, ncol = 2, nrow = 3)
figure.box

in.signif.vars.short <- signif.vars[(nrow(signif.vars) - 10):(nrow(signif.vars) - 5),]
for (i in 1:6)
{
  mytext <- paste0("i.plot",i," <-  getBoxPlot(df.train,df.train$classe,df.train$",in.signif.vars.short[i,]$COLUMN_NAME,
                   ",'','Class','",in.signif.vars.short[i,]$COLUMN_NAME,"','')")
  eval(parse(text=mytext))
  
  #print(mytext)
}
i.figure.box <- ggarrange(i.plot1, i.plot2, i.plot3, i.plot4,i.plot5,i.plot6, ncol = 3, nrow = 2)
i.figure.box


#plotGyrosX
```
Heatmap of all F Stats >= 100

Now that all are significant, let us find multi-collinearity
Let us plot all x,y,z columns against one another for a basic heat map
```{r basicxyzheatmap, fig.height= 24, fig.width=24}
f.stat.100 <- signif.vars[signif.vars$AOV_F_STATISTIC >= 100,]$COLUMN_NAME
f.stat.100 <- f.stat.100[!(f.stat.100 %in% "X")]

#xyz.vars <- colnames(df.train)[grep('_[xyz]$',colnames(df.train))]
#plot(df.train[xyz.vars])
correlation.matrix <- cor(df.train[f.stat.100])
correlation.matrix <- round(correlation.matrix,2)
correlation.matrix[upper.tri(correlation.matrix)] <- NA
diag(correlation.matrix) <- NA
row.corr.matrix <- melt(correlation.matrix,na.rm=TRUE)
colnames(row.corr.matrix) <- c("F1","F2","CORR")

heatMap <- ggplot(data = row.corr.matrix, aes(x=F1, y=F2, fill=CORR)) + 
  geom_tile(color="white")+
  labs(title = "Correlation Coefficient Heat Map",
       subtitle = "",
       y = "Features", x = "Features") + 
  scale_fill_gradient2(low = "red", high = "darkgreen", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation\nCoefficient") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  geom_text(aes(x=F1, y=F2, label = CORR), color = "black", size = 4)

heatMap
```
From the Heat Map we get some exclusions as under - 

magnet_belt_z
magnet_arm_y
accel_arm_x
accel_dumbbell_x
accel_dumbbell_y

```{r removals}
removals <- c('magnet_belt_z',
'magnet_arm_y',
'accel_arm_x',
'accel_dumbbell_x',
'accel_dumbbell_y'
)

f.stat.100 <- f.stat.100[!(f.stat.100 %in% removals)]
f.stat.100
length(f.stat.100)
```

Heat Map Post Removals

```{r basicxyzheatmappostremovals, fig.height= 24, fig.width=24}
#xyz.vars <- colnames(df.train)[grep('_[xyz]$',colnames(df.train))]
#plot(df.train[xyz.vars])
correlation.matrix <- cor(df.train[f.stat.100])
correlation.matrix <- round(correlation.matrix,2)
correlation.matrix[upper.tri(correlation.matrix)] <- NA
diag(correlation.matrix) <- NA
row.corr.matrix <- melt(correlation.matrix,na.rm=TRUE)
colnames(row.corr.matrix) <- c("F1","F2","CORR")

heatMap2 <- ggplot(data = row.corr.matrix, aes(x=F1, y=F2, fill=CORR)) + 
  geom_tile(color="white")+
  labs(title = "Correlation Coefficient Heat Map",
       subtitle = "",
       y = "Features", x = "Features") + 
  scale_fill_gradient2(low = "darkorange", high = "darkblue", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation\nCoefficient") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  geom_text(aes(x=F1, y=F2, label = CORR), color = "black", size = 4)

heatMap2
```

## We will do a 5 fold cross validation here (k = 5)
```{r basicmodels, eval=TRUE, cache=TRUE}
predictor.vars <- f.stat.100
set.seed(1234)
train_control<- trainControl(method="cv", number=5, savePredictions = TRUE)
mytext <- "myModel <- train(classe~"
mytext <- paste0(mytext,paste(predictor.vars,collapse="+"))
mytext <- paste0(mytext,",data=df.train,trControl=train_control,method='rf',trace=FALSE)")
eval(parse(text=mytext))
```

## Confusion Matrix on Cross Validation Data
```{r trainingConfusionMatrix}
model.kfold.predictions <- myModel$pred
list.predictions <- split(model.kfold.predictions,model.kfold.predictions$Resample)
cv.confusion.matrix <- lapply(list.predictions,function(df) confusionMatrix(df$pred,df$obs))
cv.confusion.matrix
```

## Let us explore the test data to see whether the predictor vars are available in the test set
```{r testDataExploration}
summary(df.test[c(predictor.vars)])
str(df.test[c(predictor.vars)])
```


```{r predictTestSet}
test.predictions <- predict(myModel,df.test)
test.predictions
df.test$prediction <- test.predictions
df.test[c("problem_id","user_name","prediction")]
```
